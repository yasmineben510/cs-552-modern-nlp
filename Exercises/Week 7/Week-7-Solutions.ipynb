{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac671356",
   "metadata": {
    "id": "ac671356"
   },
   "source": [
    "#  Exercise 7: In-Context Learning with GPT-4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc96df3",
   "metadata": {
    "id": "7fc96df3"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Exercise Description**\n",
    "In this exercise, you will investigate in-context learning using OpenAI GPT-4o mini model. \n",
    "\n",
    "This exercise contains two parts.\n",
    "\n",
    "- In the first part, you will investigate in-context learning for classification based on a natural language inference (NLI) task.\n",
    "    \n",
    "- In the second part, you will investigate in-context learning for generation based on a story ending generation (SEG) task.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **[PART 1: In-Context Learning for Natural Language Inference](#1)**\n",
    "    - [1.1 Compare Different Shots](#11)\n",
    "    - [1.2 Effect of Neutral In-Context Examples](#12)\n",
    "    - [1.3 Play with Different Verbalizers](#13)\n",
    "    - [1.4 Task Instructions](#14)\n",
    "- **[PART 2: In-Context Learning for Story Ending Generation](#2)**\n",
    "    - [2.1 Zero-Shot Generation](#21)\n",
    "    - [2.2 Few-Shot Generation](#22)\n",
    "    - [2.3 Task Instructions](#23)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLBx5JbP0xNR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17844,
     "status": "ok",
     "timestamp": 1681740155337,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "iLBx5JbP0xNR",
    "outputId": "a40cc025-2d9a-481c-e282-52b14864a6e2"
   },
   "outputs": [],
   "source": [
    "# if you are using Google Colab, mount your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# switch to the path where you put the Exercise folder into\n",
    "%cd \"/content/drive/MyDrive/...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a720ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12760,
     "status": "ok",
     "timestamp": 1681740200735,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "a9a720ab",
    "outputId": "9c92e6e6-3b81-419b-8121-34e299d7e6aa",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m967.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m951.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, numpy, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 numpy-2.2.4 regex-2024.11.6 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tqdm nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e14fd",
   "metadata": {
    "id": "623e14fd"
   },
   "source": [
    "You also need to install our **GPT wrapper** to interact with OpenAI GPT models for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c8ba5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8408,
     "status": "ok",
     "timestamp": 1681740214720,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "3d4c8ba5",
    "outputId": "d762dd09-aeb5-4731-8eae-50c520d80ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./gpt_wrapper-0.1.0-py3-none-any.whl\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m876.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-macosx_10_9_universal2.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.0/198.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests, gpt-wrapper\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 gpt-wrapper-0.1.0 idna-3.10 requests-2.32.3 urllib3-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt_wrapper-0.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0ca95",
   "metadata": {
    "id": "d4d0ca95"
   },
   "source": [
    "Import the required packages for this exercise, including our GPT wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d81dd37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1681740221134,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "8d81dd37",
    "outputId": "f726397f-6cf5-435a-86a5-7d403c8d6423"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mismayil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mismayil/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mismayil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049b54b",
   "metadata": {
    "id": "3049b54b"
   },
   "source": [
    "To facilitate reproduction, we fix a random seed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acca193e",
   "metadata": {
    "id": "acca193e"
   },
   "outputs": [],
   "source": [
    "seed = 233"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721322d2",
   "metadata": {
    "id": "721322d2"
   },
   "source": [
    "Set up the API access to our GPT wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd376243",
   "metadata": {
    "id": "cd376243"
   },
   "outputs": [],
   "source": [
    "gpt_wrapper.api_base = \"http://mnlp-backend-lb-1062233132.eu-central-1.elb.amazonaws.com\"\n",
    "gpt_wrapper.api_key = \"1067c253-7e95-42bc-9b57-fb03508f30dd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e49157",
   "metadata": {
    "id": "11e49157"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## **PART 1: In-Context Learning for Natural Language Inference**\n",
    "---\n",
    "\n",
    "In this part, you are going to use the GPT-4o mini model to solve the [natural language inference (NLI)](https://towardsdatascience.com/natural-language-inference-an-overview-57c0eecf6517) task based on in-context learning. For this task, model needs to classify the relation of two given sentences (premise and hypothesis) into three classes: entailment, neutral and contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed303c88",
   "metadata": {
    "id": "ed303c88"
   },
   "source": [
    "Here you can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-4o mini language model for classification (along with the gold answers for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ee990d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1681740230765,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "f8ee990d",
    "outputId": "b35316c7-6c25-421a-bce4-8f77b5b4c5a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:\n",
      "I know lawyers are always dreadfully careful.\n",
      "I'm well aware that lawyers are always very careful.\n",
      "Answer: entailment\n",
      "\n",
      "\n",
      "Testing Query:\n",
      "The new rights are nice enough\n",
      "Everyone really likes the newest benefits \n",
      "Answer:\n",
      "\n",
      "\n",
      "Gold Answer:\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "with open(\"nli_classification/train_classification.json\", \"r\") as f:\n",
    "    train_samples = json.load(f)\n",
    "with open(\"nli_classification/test_classification.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(\"Training Samples:\")\n",
    "print(train_samples[\"entailment\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caddef",
   "metadata": {
    "id": "01caddef"
   },
   "source": [
    "As you know by now, language models only produce a probability distribution over the vocabulary, so how many tokens to generate and how to select each token depends on the decoding algorithm. Since we are going to use GPT-4o mini model, we should get familiar with what type of decoding parameters OpenAI API offers:\n",
    "\n",
    "**max_tokens**: Maximum number of tokens to generate, by default generates as many as needed.\n",
    "\n",
    "**temperature**: Sampling temperature to use, between 0.0 and 2.0, default to 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "**top_p**: Nucleus sampling factor (alternative to sampling with temperature), between 0.0 and 1.0, default to 1.0. The model randomly samples from the tokens with top_p probability mass.\n",
    "\n",
    "**presence_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "\n",
    "**frequency_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HN3X2CRHdUmW",
   "metadata": {
    "id": "HN3X2CRHdUmW"
   },
   "source": [
    "For NLI task, we choose a small *max_tokens* because only the first non-space token generated by the model is used as the predicted class (i.e., verbalizer).\n",
    "\n",
    "**Question:**\n",
    "Why don't we simply set it to 1?\n",
    "\n",
    "**Reference answer:** Because OpenAI models operate over subword level tokenizers, hence long words are typically broken into several tokens such as contradiction=\"contrad\" + \"iction\". \n",
    "\n",
    "We also change the *temperature* to 0 in order to let the model make deterministic classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ab3bc651",
   "metadata": {
    "id": "ab3bc651"
   },
   "outputs": [],
   "source": [
    "model_args={\"max_tokens\": 5, \"temperature\": 0.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab97e5d",
   "metadata": {
    "id": "4ab97e5d"
   },
   "source": [
    "You will evaluate the model's NLI performance based on the accuracy and F1 scores on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6129f62",
   "metadata": {
    "id": "f6129f62"
   },
   "outputs": [],
   "source": [
    "def evaluate_nli(predictions, gold_labels, mapping):\n",
    "    \n",
    "    counter = np.zeros((3, 3))  # three-class confusion matrix\n",
    "    \n",
    "    # calculate the confusion matrix\n",
    "    for p, g in zip(predictions, gold_labels):\n",
    "        pid = mapping[p]\n",
    "        gid = mapping[g]\n",
    "        counter[gid][pid] += 1\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(counter)\n",
    "    \n",
    "    pred_sum = np.sum(counter, axis=0)  # total number of predictions on each class\n",
    "    gold_sum = np.sum(counter, axis=1)  # total number of test samples (gold labels) on each class\n",
    "    diag = np.diagonal(counter)  # total number of correct predictions on each class\n",
    "    \n",
    "    acc = np.sum(diag) / np.sum(counter)  # accuracy\n",
    "    \n",
    "    f1 = [0, 0, 0]\n",
    "    for cid in range(3):\n",
    "        precision = diag[cid] / pred_sum[cid] if pred_sum[cid] != 0 else 0  # precisions on each class\n",
    "        recall = diag[cid] / gold_sum[cid] if gold_sum[cid] != 0 else 0  # recalls on each class\n",
    "        f1[cid] = 2 * precision * recall / (precision + recall) if precision != 0 and recall != 0 else 0  # F1 scores on each class\n",
    "    \n",
    "    return acc, f1[0], f1[1], f1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886b1c6",
   "metadata": {
    "id": "d886b1c6"
   },
   "source": [
    "You will use the following function to perform GPT-4o mini inference on the NLI task based on in-context learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a878781",
   "metadata": {
    "id": "7a878781"
   },
   "outputs": [],
   "source": [
    "def gpt_nli(train_samples, test_data, shots, predictions, gold_answers,\n",
    "             instruction=None, default_class=\"neutral\", task_name=\"task\"):\n",
    "    \n",
    "    '''\n",
    "    train_samples: training data for sampling in-context examples\n",
    "    test_data: testing queries (with gold labels)\n",
    "    shots: number of in-context examples (shots) per class\n",
    "    predictions: cache for saving the model predictions\n",
    "    gold_answers: cache for saving gold answers\n",
    "    instruction: additional task instruction for prompting\n",
    "    default_class: default prediction class if the generated token is not among the verbalizers of three NLI classes\n",
    "    task_name: task name for creating chat sessions\n",
    "    '''\n",
    "    \n",
    "    # randomly sample in-context examples\n",
    "    examples = []\n",
    "    for nli_class, samples in train_samples.items():\n",
    "        few_shot_samples = random.sample(samples, shots[nli_class])\n",
    "        examples.extend(few_shot_samples)\n",
    "\n",
    "    random.shuffle(examples)  # randomly shuffle sampled in-context examples\n",
    "\n",
    "    for qid, query in enumerate(tqdm(test_data)):\n",
    "        \n",
    "        if qid < len(predictions):  # skip this query if its model prediction is already saved in cache\n",
    "            continue\n",
    "\n",
    "        # TODO: Construct the prompt using the in-context examples and the testing query\n",
    "        prompt = \"\\n\\n\".join(examples+[query[\"query\"]])\n",
    "\n",
    "        # create a chat session using our GPT wrapper class Chat\n",
    "        chat = Chat.create(name=task_name+\"_\"+str(qid))\n",
    "        \n",
    "        # use the created chat session to query the GPT model with the input request,\n",
    "        # and get back model's output message\n",
    "        response = chat.ask(prompt, instruction=instruction, model_args=model_args)\n",
    "        \n",
    "        # model's output text is in the attribute \"content\",\n",
    "        # we use the first word of the generated text as the prediction\n",
    "        preds = response.content.strip().split()\n",
    "\n",
    "        if preds:\n",
    "            pred = preds[0].lower()\n",
    "        else:\n",
    "            pred = default_class\n",
    "        \n",
    "        # save the prediction in cache\n",
    "        if pred in train_samples.keys():\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            predictions.append(default_class)\n",
    "        \n",
    "        # save the gold answer in cache for evaluation\n",
    "        gold_answers.append(query[\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6619024",
   "metadata": {
    "id": "a6619024"
   },
   "source": [
    "You will run the following function to perform GPT inference and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "692af4be",
   "metadata": {
    "id": "692af4be"
   },
   "outputs": [],
   "source": [
    "def run(train_samples, test_data, class_shots, mapping, predictions, gold_answers,\n",
    "        instruction=None, default_class=\"neutral\", task_name=\"none\"):\n",
    "    try:\n",
    "\n",
    "        gpt_nli(train_samples, test_data, class_shots, predictions, gold_answers,\n",
    "                 instruction=instruction, default_class=default_class, task_name=task_name)\n",
    "        \n",
    "        acc, f1_ent, f1_neu, f1_con = evaluate_nli(predictions, gold_answers, mapping)\n",
    "        macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n",
    "\n",
    "        print(f'Accuracy: {acc*100:.2f}% | F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')\n",
    "\n",
    "    except Exception as error:  # OpenAI ChatGPT endpoint may get stucked by too many queries from time to time\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ea96b",
   "metadata": {
    "id": "c90ea96b"
   },
   "source": [
    "<a name=\"11\"></a>\n",
    "### **1.1 Compare Different Shots**\n",
    "\n",
    "In this part, you will compare GPT-4o mini performances under different shots (number) of in-context examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25d6a1",
   "metadata": {
    "id": "2f25d6a1"
   },
   "source": [
    "#### 0-shot classification:\n",
    "\n",
    "Do not provide any in-context learning examples to the model.\n",
    "\n",
    "Create empty caches for saving model predictions and gold answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ae867da",
   "metadata": {
    "id": "0ae867da"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921da49",
   "metadata": {
    "id": "3921da49"
   },
   "source": [
    "Run the inference and evaluation.\n",
    "\n",
    "**Note:** OpenAI ChatGPT endpoint may sometimes get stucked by too many queries. If running the following cell gets stucked, just re-run it, and inference will continue from the stucked query. However, do not re-run the above cell for creating the caches, which will clear the already saved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78fa56d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22585,
     "status": "ok",
     "timestamp": 1681740282153,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "78fa56d7",
    "outputId": "1eea57e7-128d-4d68-e877-b3910de50084"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:26<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 0. 10.  0.]\n",
      " [ 0. 10.  0.]\n",
      " [ 0. 10.  0.]]\n",
      "Accuracy: 33.33% | F1: (0.00%, 50.00%, 0.00%) | Macro-F1: 16.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 0, \"neutral\": 0, \"contradiction\": 0}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158536b5",
   "metadata": {
    "id": "158536b5"
   },
   "source": [
    "#### 1-shot per class:\n",
    "\n",
    "For each class, provide 1 in-context learning example sampled from the training data.\n",
    "\n",
    "Clear the caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6bd54af4",
   "metadata": {
    "id": "6bd54af4"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cb520",
   "metadata": {
    "id": "a43cb520"
   },
   "source": [
    "Re-run the inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f544c0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23085,
     "status": "ok",
     "timestamp": 1681740311281,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "f544c0fd",
    "outputId": "655c95a4-1be4-45f0-d519-8fda494f85a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:24<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8. 2. 0.]\n",
      " [4. 5. 1.]\n",
      " [1. 0. 9.]]\n",
      "Accuracy: 73.33% | F1: (69.57%, 58.82%, 90.00%) | Macro-F1: 72.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2eb071",
   "metadata": {
    "id": "1f2eb071"
   },
   "source": [
    "#### 2-shot per class:\n",
    "\n",
    "Try 2 in-context learning examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae49ee1",
   "metadata": {
    "id": "3ae49ee1"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143c7eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23763,
     "status": "ok",
     "timestamp": 1681740340798,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "143c7eba",
    "outputId": "12d893fb-401c-433a-fd47-b77128db9275"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8. 2. 0.]\n",
      " [2. 7. 1.]\n",
      " [1. 0. 9.]]\n",
      "Accuracy: 80.00% | F1: (76.19%, 73.68%, 90.00%) | Macro-F1: 79.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 2, \"neutral\": 2, \"contradiction\": 2}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f41c3",
   "metadata": {
    "id": "ae6f41c3"
   },
   "source": [
    "#### 3-shot per class:\n",
    "\n",
    "Try 3 in-context learning examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c86dc6c2",
   "metadata": {
    "id": "c86dc6c2"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32278700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22961,
     "status": "ok",
     "timestamp": 1681740372517,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "32278700",
    "outputId": "a8c0c8e6-ccb2-4912-d3d7-672f0ebe05a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:28<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[9. 1. 0.]\n",
      " [3. 2. 5.]\n",
      " [1. 0. 9.]]\n",
      "Accuracy: 66.67% | F1: (78.26%, 30.77%, 75.00%) | Macro-F1: 61.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 3, \"neutral\": 3, \"contradiction\": 3}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca3308",
   "metadata": {
    "id": "fcca3308"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Can model handle well the NLI task without in-context examples for learning (i.e., under the 0-shot setting)?\n",
    "2. Which class are the in-context examples most helpful for detecting? and most helpless?\n",
    "3. Is the more in-context examples the better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecfe41",
   "metadata": {},
   "source": [
    "**Reference Answers:**\n",
    "\n",
    "1. No, because the model cannot learn to generate the required verbalizers (i.e., entailment, neutral and contradiction) during the classification, so the predictions are always the default class.\n",
    "2. In-context examples are most helpful in detecting the contradiction class, while most helpless in detecting the neutral class, probably because neutral samples are more prone to be identified as having some entailed or contradicted relations.\n",
    "3. No, more in-context examples does not necessarily lead to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87231e",
   "metadata": {
    "id": "ae87231e"
   },
   "source": [
    "<a name=\"12\"></a>\n",
    "### **1.2 Effect of Neutral In-Context Examples**\n",
    "\n",
    "Try 3-shot in-context examples on the entailment and contradictions classes, but do not provide any examples on the neutral class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d78c89",
   "metadata": {
    "id": "f6d78c89"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae734042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23782,
     "status": "ok",
     "timestamp": 1681740411286,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "ae734042",
    "outputId": "16e483c8-122e-48bb-bd26-74d371cb136f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:21<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10.  0.  0.]\n",
      " [ 6.  1.  3.]\n",
      " [ 1.  0.  9.]]\n",
      "Accuracy: 66.67% | F1: (74.07%, 18.18%, 81.82%) | Macro-F1: 58.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 3, \"neutral\": 0, \"contradiction\": 3}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef6a43",
   "metadata": {
    "id": "e8ef6a43"
   },
   "source": [
    "**Question:** Do you observe any difference here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108eca5f",
   "metadata": {},
   "source": [
    "**Reference Answer:** The model's performance on detecting entailment queries is slightly improved and this shows that neutral examples were perhaps confusing for the model. Now that there are no demonstrations for neutral label, model tends to output only entailment or contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c56bf9",
   "metadata": {
    "id": "77c56bf9"
   },
   "source": [
    "<a name=\"13\"></a>\n",
    "### **1.3 Play with Different Verbalizers**\n",
    "\n",
    "In this part, you will try to use different verbalizers for this NLI classification task. Verbalizers are mapping functions that map the numeric class labels such as 0, 1 and 2 to human-readable class labels such \"entailment\", \"neutral\" and \"contradiction\". However, this mapping is by no means the only correct one and in theory, we can use any mapping we want. So, next instead of using *entailment*, *neutral* and *contradiction*, you will try the following two alternatives:\n",
    "\n",
    "- *positive*, *unrelated* and *negative*\n",
    "- *a*, *b* and *c*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7e4f2",
   "metadata": {
    "id": "f9f7e4f2"
   },
   "source": [
    "Build data with the above two different verbalizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f33a23",
   "metadata": {
    "id": "88f33a23"
   },
   "outputs": [],
   "source": [
    "mapping_to_pun = {\"entailment\": \"positive\", \"neutral\": \"unrelated\", \"contradiction\": \"negative\"}\n",
    "train_samples_pun = {\"positive\": [], \"unrelated\": [], \"negative\": []}\n",
    "test_data_pun = []\n",
    "\n",
    "mapping_to_abc = {\"entailment\": \"a\", \"neutral\": \"b\", \"contradiction\": \"c\"}\n",
    "train_samples_abc = {\"a\": [], \"b\": [], \"c\": []}\n",
    "test_data_abc = []\n",
    "\n",
    "for nli_class, samples in train_samples.items():\n",
    "    # TODO: Populate the new training data for the two new tasks where the NLI classes are verbalized as \"positive\", \"unrelated\", and \"negative\",\n",
    "    # and as \"a\", \"b\", and \"c\", respectively\n",
    "    nli_class_pun = mapping_to_pun[nli_class]\n",
    "    nli_class_abc = mapping_to_abc[nli_class]\n",
    "    \n",
    "    for sample in samples:\n",
    "        \n",
    "        sample_pun = \" \".join(sample.split(\" \")[:-1] + [nli_class_pun])\n",
    "        train_samples_pun[nli_class_pun].append(sample_pun)\n",
    "        \n",
    "        sample_abc = \" \".join(sample.split(\" \")[:-1] + [nli_class_abc])\n",
    "        train_samples_abc[nli_class_abc].append(sample_abc)\n",
    "    \n",
    "for query in test_data:\n",
    "    # TODO: Populate the new testing data for the two new tasks where the gold answers are verbalized as \"positive\", \"unrelated\", and \"negative\",\n",
    "    # and as \"a\", \"b\", and \"c\", respectively\n",
    "    query_pun = deepcopy(query)\n",
    "    query_pun[\"gold_answer\"] = mapping_to_pun[query[\"gold_answer\"]]\n",
    "    test_data_pun.append(query_pun)\n",
    "    \n",
    "    query_abc = deepcopy(query)\n",
    "    query_abc[\"gold_answer\"] = mapping_to_abc[query[\"gold_answer\"]]\n",
    "    test_data_abc.append(query_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11b99e",
   "metadata": {
    "id": "fc11b99e"
   },
   "source": [
    "You can take a glance of the processed training and testing data with different verbalizers.\n",
    "\n",
    "Data with verbalizers *positive*, *unrelated* and *negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae7c0de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1681740449664,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "7ae7c0de",
    "outputId": "29004591-1864-44f3-e250-6bbbbba1cd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:\n",
      "I know lawyers are always dreadfully careful.\n",
      "I'm well aware that lawyers are always very careful.\n",
      "Answer: positive\n",
      "\n",
      "\n",
      "Testing Query:\n",
      "The new rights are nice enough\n",
      "Everyone really likes the newest benefits \n",
      "Answer:\n",
      "\n",
      "\n",
      "Gold Answer:\n",
      "unrelated\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Samples:\")\n",
    "print(train_samples_pun[\"positive\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_pun[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data_pun[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4783fca",
   "metadata": {
    "id": "a4783fca"
   },
   "source": [
    "Data with verbalizers *a*, *b* and *c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f64c3cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681740451809,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "2f64c3cb",
    "outputId": "39cfc7d3-453f-4b17-d6dd-9faca32ff8f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:\n",
      "I know lawyers are always dreadfully careful.\n",
      "I'm well aware that lawyers are always very careful.\n",
      "Answer: a\n",
      "\n",
      "\n",
      "Testing Query:\n",
      "The new rights are nice enough\n",
      "Everyone really likes the newest benefits \n",
      "Answer:\n",
      "\n",
      "\n",
      "Gold Answer:\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Samples:\")\n",
    "print(train_samples_abc[\"a\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_abc[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data_abc[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30fd47",
   "metadata": {
    "id": "eb30fd47"
   },
   "source": [
    "#### Re-do the classification with new verbalizers.\n",
    "\n",
    "Try verbalizers *positive*, *unrelated* and *negative* under the 2-shot setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14b52710",
   "metadata": {
    "id": "14b52710"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3842ede7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23845,
     "status": "ok",
     "timestamp": 1681740485335,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "3842ede7",
    "outputId": "c83cffae-6cfd-4d6d-9894-eca326ba7e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:23<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8. 1. 1.]\n",
      " [3. 1. 6.]\n",
      " [1. 0. 9.]]\n",
      "Accuracy: 60.00% | F1: (72.73%, 16.67%, 69.23%) | Macro-F1: 52.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots_pun = {\"positive\": 2, \"unrelated\": 2, \"negative\": 2}\n",
    "mapping_pun = {\"positive\": 0, \"unrelated\": 1, \"negative\": 2}\n",
    "\n",
    "run(train_samples_pun, test_data_pun, class_shots_pun, mapping_pun,\n",
    "    predictions, gold_answers, default_class=\"unrelated\", task_name=\"1_3_pun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada009a",
   "metadata": {
    "id": "5ada009a"
   },
   "source": [
    "Try verbalizers *a*, *b* and *c* under the 2-shot setting in 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1efccae",
   "metadata": {
    "id": "f1efccae"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df8194f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9311,
     "status": "ok",
     "timestamp": 1681740556153,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "df8194f6",
    "outputId": "d90bc77a-43cb-42b3-ab1b-df2daf9ccedf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:22<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[5. 4. 1.]\n",
      " [0. 5. 5.]\n",
      " [0. 1. 9.]]\n",
      "Accuracy: 63.33% | F1: (66.67%, 50.00%, 72.00%) | Macro-F1: 62.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots_abc = {\"a\": 2, \"b\": 2, \"c\": 2}\n",
    "mapping_abc = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "\n",
    "run(train_samples_abc, test_data_abc, class_shots_abc, mapping_abc,\n",
    "    predictions, gold_answers, default_class=\"b\", task_name=\"1_3_abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f384d",
   "metadata": {
    "id": "2c6f384d"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Are new verbalizers better or worse than the original ones?\n",
    "2. What do you think could be the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cab5e",
   "metadata": {},
   "source": [
    "**Reference answers:**\n",
    "1. They both seem to be worse in accuracy than the original class labels in 2-shot setting.\n",
    "2. One reason could be that the new class labels are either less relevant (positive, negative, unrelated) or not related at all (a,b,c) to the task at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c62887",
   "metadata": {
    "id": "77c62887"
   },
   "source": [
    "<a name=\"14\"></a>\n",
    "### **1.4 Task Instructions**\n",
    "\n",
    "So far, we have only provided the model with a few examples of the NLI task, however, we didn't tell the model explicitly what the task is! But we also know that these models are self-supervised with a lot of data to follow human instructions to solve tasks such as \"Translate English to French\" or \"Answer in the style of Shakespeare\". \n",
    "\n",
    "In this part, you will try to come up with high-level task instructions/descriptions that can help the model understand the NLI task better and improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f123b9cb",
   "metadata": {
    "id": "f123b9cb"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8252537f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23221,
     "status": "ok",
     "timestamp": 1681740593777,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "8252537f",
    "outputId": "6ca69e25-0d69-4b88-ec62-9b13fa9dc992"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:23<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8. 2. 0.]\n",
      " [4. 6. 0.]\n",
      " [1. 1. 8.]]\n",
      "Accuracy: 73.33% | F1: (69.57%, 63.16%, 88.89%) | Macro-F1: 73.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TODO: Come up with a simple task description for the NLI task. You can also consider adding a role-playing element to the task description.\n",
    "instruction = \"Guess whether the second statement is entailed by the first statement, contradicts the first statement, or is neutral to the first statement.\"\n",
    "class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping,\n",
    "    predictions, gold_answers, instruction=instruction, task_name=\"1_4_intro1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d0159",
   "metadata": {
    "id": "f17d0159"
   },
   "source": [
    "**Question:** What kind of instruction help improve the model performance and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f6131",
   "metadata": {
    "id": "5c8f6131"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## **PART 2: In-Context Learning for Story Ending Generation**\n",
    "---\n",
    "\n",
    "In this part, you will switch to using the GPT-4o mini model to solve the story ending generation (SEG) task based on in-context learning. For this task, model is given four lines of story plot and needs to generate the fifth line of the story plot as an ending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52dacd",
   "metadata": {
    "id": "2f52dacd"
   },
   "source": [
    "You can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-4o mini language model for story completion (along with the reference story ending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d31a9bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1681740778880,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "2d31a9bd",
    "outputId": "9fa6d309-b0d7-4f2f-8f73-5647ff995391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:\n",
      "Dan's parents were overweight.\n",
      "Dan was overweight as well.\n",
      "The doctors told his parents it was unhealthy.\n",
      "His parents understood and decided to make a change.\n",
      "Output: They got themselves and Dan on a diet.\n",
      "\n",
      "\n",
      "Testing Query:\n",
      "A few days ago I decided to take my dog Sable for a walk.\n",
      "She is a half-pit bull half-bulldog with a lot of strength.\n",
      "After I got her leash on I opened the garage to head outside.\n",
      "She tried bolting out of the garage and dragged me along with her.\n",
      "Output:\n",
      "\n",
      "\n",
      "Reference Story Ending:\n",
      "After the half hour walk my muscles were sore as if I had worked out.\n"
     ]
    }
   ],
   "source": [
    "with open(\"story_generation/train_generation.json\", \"r\") as f:\n",
    "    train_samples_sg = json.load(f)\n",
    "with open(\"story_generation/test_generation.json\", \"r\") as f:\n",
    "    test_data_sg = json.load(f)\n",
    "\n",
    "print(\"Training Samples:\")\n",
    "print(train_samples_sg[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_sg[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Reference Story Ending:\")\n",
    "print(test_data_sg[0][\"reference_ending\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a56277",
   "metadata": {
    "id": "a8a56277"
   },
   "source": [
    "We will remove the limit on the maximum number of tokens since we are dealing with an open-ended text generation.\n",
    "\n",
    "We also change the *temperature* and *top_p* to 0.7 and 0.95 in order to enable the model's creativity and make it generate more diverse story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "489af8bb",
   "metadata": {
    "id": "489af8bb"
   },
   "outputs": [],
   "source": [
    "model_args = {\"temperature\": 0.7, \"top_p\": 0.95, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fcc7f",
   "metadata": {
    "id": "d59fcc7f"
   },
   "source": [
    "You will evaluate the model generation performance based on [METEOR](https://aclanthology.org/W05-0909.pdf). This metric is originally proposed to evaluate machine translation quality, but later widely used in evaluating open-domain text (e.g., dialogues and stories) generation. It measures the alignments (i.e., matches) between words in the hypothesis to reference, by sequentially applying exact match, stemmed match and wordnet based synonym match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "93f4a599",
   "metadata": {
    "id": "93f4a599"
   },
   "outputs": [],
   "source": [
    "def evaluate_seg(generation, reference):\n",
    "    ref_tokens = word_tokenize(reference)\n",
    "    gen_tokens = word_tokenize(generation)\n",
    "    score = meteor_score([ref_tokens], gen_tokens)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69d0d4",
   "metadata": {
    "id": "ff69d0d4"
   },
   "source": [
    "You will use the following function to perform GPT-4o mini generation on the SEG task based on in-context learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "352adb17",
   "metadata": {
    "id": "352adb17"
   },
   "outputs": [],
   "source": [
    "def gpt_seg(train_samples, test_data, shot, generations, queries, reference_endings,\n",
    "             instruction=None, task_name=\"task\"):\n",
    "\n",
    "    '''\n",
    "    train_samples: training data for sampling in-context examples\n",
    "    test_data: testing queries (with reference story endings)\n",
    "    shot: number of in-context examples\n",
    "    generations: cache for saving the model generations\n",
    "    queries: cache for saving the input queries (i.e., four-line stories to be completed)\n",
    "    reference_endings: cache for reference story endings\n",
    "    instruction: additional task instruction for prompting\n",
    "    task_name: task name for creating chat sessions\n",
    "    '''\n",
    "    \n",
    "    # randomly sample in-context examples and shuffle them\n",
    "    examples = random.sample(train_samples, shot)\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    for qid, query in enumerate(tqdm(test_data)):\n",
    "        \n",
    "        if qid < len(generations):  # skip this query if its model generated story ending is already saved in cache\n",
    "            continue\n",
    "\n",
    "        # TODO: Construct the prompt using the in-context examples and the testing query\n",
    "        prompt = \"\\n\\n\".join(examples+[query[\"query\"]])\n",
    "\n",
    "        # create a chat session using our GPT wrapper and query the model to get the story ending generation\n",
    "        chat = Chat.create(name=task_name+\"_\"+str(qid))\n",
    "        message = chat.ask(prompt, instruction=instruction, model_args=model_args)\n",
    "        \n",
    "        # save the model generation, story query and reference ending in caches\n",
    "        generations.append(message.content)\n",
    "        queries.append(query[\"query\"])\n",
    "        reference_endings.append(query[\"reference_ending\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28953b",
   "metadata": {
    "id": "5b28953b"
   },
   "source": [
    "You will run the following function to perform GPT-4o mini generation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e696a",
   "metadata": {
    "id": "fe1e696a"
   },
   "outputs": [],
   "source": [
    "def run(train_samples, test_data, shot, generations, queries, reference_endings, instruction=None, task_name=\"none\"):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        gpt_seg(train_samples, test_data, shot,\n",
    "                 generations, queries, reference_endings,\n",
    "                 instruction=instruction, task_name=task_name)\n",
    "\n",
    "        meteor_scores = []\n",
    "        print()\n",
    "\n",
    "        for qid, query in enumerate(queries):\n",
    "\n",
    "            meteor = evaluate_seg(generations[qid], reference_endings[qid])\n",
    "            print(\"Query \"+str(qid+1)+f' METEOR Score: {meteor*100:.2f}') \n",
    "\n",
    "            meteor_scores.append(meteor)\n",
    "\n",
    "        meteor_avg = sum(meteor_scores)/len(meteor_scores)\n",
    "        print(f'Average METEOR Score: {meteor_avg*100:.2f}')\n",
    "    \n",
    "    except Exception as error:  # OpenAI ChatGPT endpoint may get stucked by too many queries from time to time\n",
    "        \n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabe9f8",
   "metadata": {
    "id": "2aabe9f8"
   },
   "source": [
    "<a name=\"21\"></a>\n",
    "### **2.1 Zero-Shot Generation**\n",
    "\n",
    "Try 0-shot story ending generation (i.e., without any in-context examples).\n",
    "\n",
    "Create caches for saving model predictions, queries and reference story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff383572",
   "metadata": {
    "id": "ff383572"
   },
   "outputs": [],
   "source": [
    "generations_21 = []\n",
    "queries_21 = []\n",
    "reference_endings_21 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58400cf",
   "metadata": {
    "id": "c58400cf"
   },
   "source": [
    "Run the generation and evaluation.\n",
    "\n",
    "**Note:** Similar to Part 1, re-run the cell if it gets stucked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc252e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16075,
     "status": "ok",
     "timestamp": 1681741289721,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "bc252e8e",
    "outputId": "b72dc55f-e85a-4b7d-d1ea-195dcd8d6e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1 METEOR Score: 11.51\n",
      "Query 2 METEOR Score: 21.05\n",
      "Query 3 METEOR Score: 37.66\n",
      "Query 4 METEOR Score: 15.60\n",
      "Query 5 METEOR Score: 13.51\n",
      "Query 6 METEOR Score: 34.95\n",
      "Query 7 METEOR Score: 8.43\n",
      "Query 8 METEOR Score: 21.80\n",
      "Query 9 METEOR Score: 12.62\n",
      "Query 10 METEOR Score: 17.71\n",
      "Average METEOR Score: 19.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "run(train_samples_sg, test_data_sg, 0, generations_21, queries_21, reference_endings_21, task_name=\"2_1_shots0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028cff13",
   "metadata": {
    "id": "028cff13"
   },
   "source": [
    "You can print the saved caches and compare the quality of the reference and model-generated story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e0b2dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1681741384000,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "6e0b2dd1",
    "outputId": "be324930-dd77-4254-8b2c-c4e8c7391a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      "A few days ago I decided to take my dog Sable for a walk.\n",
      "She is a half-pit bull half-bulldog with a lot of strength.\n",
      "After I got her leash on I opened the garage to head outside.\n",
      "She tried bolting out of the garage and dragged me along with her.\n",
      "Output:\n",
      "GPT-4o mini Generation:\n",
      "As soon as I opened the garage door, Sable’s excitement was palpable. Her muscles tensed as she lunged forward, eager to explore the world outside. Before I could even get a firm grip on the leash, she took off, pulling me along like I was a ragdoll.\n",
      "\n",
      "I stumbled for a moment, trying to regain my balance while calling out her name, “Sable, slow down!” But she was too caught up in the thrill of the moment, her tail wagging furiously as she darted toward the open space of the driveway. \n",
      "\n",
      "Despite my initial surprise, I couldn’t help but laugh at her enthusiasm. Once I found my footing, I managed to pull her back slightly, reminding her that it was important to walk nicely. After a little coaxing, she settled down into a more manageable pace, though the energy still radiated from her.\n",
      "\n",
      "As we finally stepped out into the fresh air, I felt the warmth of the sun on my face, and Sable seemed to bask in it too. We walked together, her sniffing every inch of grass and pavement, exploring the neighborhood with uncontainable joy. It was a reminder of how much fun it was to take her for walks, and I couldn’t wait for our next adventure!\n",
      "Reference:\n",
      "After the half hour walk my muscles were sore as if I had worked out.\n"
     ]
    }
   ],
   "source": [
    "print(\"Queries:\\n\"+queries_21[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_21[0])\n",
    "print(\"Reference:\\n\"+reference_endings_21[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4813db",
   "metadata": {
    "id": "ae4813db"
   },
   "source": [
    "<a name=\"22\"></a>\n",
    "### **2.2 Few-Shot Generation**\n",
    "\n",
    "Try adding 5-shot in-context examples for this generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89a6f67a",
   "metadata": {
    "id": "89a6f67a"
   },
   "outputs": [],
   "source": [
    "generations_22 = []\n",
    "queries_22 = []\n",
    "reference_endings_22 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5267b3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12302,
     "status": "ok",
     "timestamp": 1681741422976,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "5267b3c5",
    "outputId": "d0486b7b-7335-426a-979b-07db5765220a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1 METEOR Score: 16.28\n",
      "Query 2 METEOR Score: 17.54\n",
      "Query 3 METEOR Score: 5.21\n",
      "Query 4 METEOR Score: 7.41\n",
      "Query 5 METEOR Score: 42.95\n",
      "Query 6 METEOR Score: 70.31\n",
      "Query 7 METEOR Score: 14.02\n",
      "Query 8 METEOR Score: 7.41\n",
      "Query 9 METEOR Score: 67.24\n",
      "Query 10 METEOR Score: 10.00\n",
      "Average METEOR Score: 25.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "run(train_samples_sg, test_data_sg, 5, generations_22, queries_22, reference_endings_22, task_name=\"2_2_shots5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be2268",
   "metadata": {
    "id": "44be2268"
   },
   "source": [
    "**Question:** Do few-shot examples improve the model's story ending generation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd1485",
   "metadata": {
    "id": "c9bd1485"
   },
   "source": [
    "You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e028b9a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1681741525710,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "e028b9a4",
    "outputId": "1730a155-1fa6-47c9-8b9f-97a5b3a67edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      "A few days ago I decided to take my dog Sable for a walk.\n",
      "She is a half-pit bull half-bulldog with a lot of strength.\n",
      "After I got her leash on I opened the garage to head outside.\n",
      "She tried bolting out of the garage and dragged me along with her.\n",
      "Output:\n",
      "GPT-3.5 Generation:\n",
      "I had to regain control of her before we got too far.\n",
      "Reference:\n",
      "After the half hour walk my muscles were sore as if I had worked out.\n"
     ]
    }
   ],
   "source": [
    "print(\"Queries:\\n\"+queries_22[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_22[0])\n",
    "print(\"Reference:\\n\"+reference_endings_22[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276b631",
   "metadata": {
    "id": "2276b631"
   },
   "source": [
    "<a name=\"23\"></a>\n",
    "### **2.3 Add Instructions**\n",
    "\n",
    "Try 1-shot in-context learning with overall task instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7bcd744",
   "metadata": {
    "id": "e7bcd744"
   },
   "outputs": [],
   "source": [
    "generations_23 = []\n",
    "queries_23 = []\n",
    "reference_endings_23 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efb203a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1681741562163,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "efb203a4",
    "outputId": "5ffac98a-d961-460a-e22c-165f08000f7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:38<00:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1 METEOR Score: 11.73\n",
      "Query 2 METEOR Score: 12.78\n",
      "Query 3 METEOR Score: 10.20\n",
      "Query 4 METEOR Score: 19.31\n",
      "Query 5 METEOR Score: 15.56\n",
      "Query 6 METEOR Score: 14.77\n",
      "Query 7 METEOR Score: 13.61\n",
      "Query 8 METEOR Score: 12.41\n",
      "Query 9 METEOR Score: 9.66\n",
      "Query 10 METEOR Score: 6.91\n",
      "Average METEOR Score: 12.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TODO: Come up with a simple task description for the story generation task. You can also consider adding a role-playing element to the task deåscription.\n",
    "instruction = \"You are a creative storyteller. Generate an ending of the given story.\"\n",
    "run(train_samples_sg, test_data_sg, 1, generations_23, queries_23, reference_endings_23,\n",
    "    instruction=instruction, task_name=\"2_3_intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd95e36",
   "metadata": {
    "id": "2cd95e36"
   },
   "source": [
    "**Question:** Does overall task instruction help improve the model's 1-shot story ending generation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314a4db",
   "metadata": {
    "id": "a314a4db"
   },
   "source": [
    "You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75485c15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681741590062,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "75485c15",
    "outputId": "0688a90b-d4b8-4edb-e70b-0f49656bbe6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      "A few days ago I decided to take my dog Sable for a walk.\n",
      "She is a half-pit bull half-bulldog with a lot of strength.\n",
      "After I got her leash on I opened the garage to head outside.\n",
      "She tried bolting out of the garage and dragged me along with her.\n",
      "Output:\n",
      "GPT-4o mini Generation:\n",
      "We stumbled out into the bright sunlight, Sable's energy pulling me forward like a freight train. I laughed, a mix of surprise and joy, as she dashed down the sidewalk, her tail wagging furiously behind her. I quickly regained my footing and called out to her, “Slow down, girl!” \n",
      "\n",
      "But Sable was caught up in the thrill of the moment, her playful spirit leading the way. We raced past neighbors and their yards, her excitement infectious. I felt the stress of the day melt away as we explored our familiar neighborhood, discovering new scents and sights that seemed to change with every walk.\n",
      "\n",
      "As the sun began to dip below the horizon, casting a golden glow over everything, Sable finally slowed down, panting happily. We turned a corner and came upon a little park, its grassy expanse inviting us in. I decided it was time for a break. \n",
      "\n",
      "I unclipped her leash and watched as she bounded into the open space, rolling in the grass and chasing after a butterfly. I smiled, knowing that the joy of the day had made it all worth it. After a while, I called her back, and she came trotting over, her ears perked up and her eyes bright with happiness. \n",
      "\n",
      "As we walked back home, the sky painted with hues of pink and orange, I realized that it was these simple moments that mattered the most. Sable, my loyal companion, had not only taken me for a walk but had also reminded me of the beauty of spontaneity and joy. By the time we arrived home, both of us were tired but content, ready to curl up together and relive our day's adventure in dreams.\n",
      "Reference:\n",
      "After the half hour walk my muscles were sore as if I had worked out.\n"
     ]
    }
   ],
   "source": [
    "print(\"Queries:\\n\"+queries_23[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_23[0])\n",
    "print(\"Reference:\\n\"+reference_endings_23[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f18dce",
   "metadata": {},
   "source": [
    "**Question**: Depending on your instruction, do you observe any undesirable pattern in the model generations when 0 or 1 shot examples are given? How can we avoid this pattern in model generations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1e313",
   "metadata": {},
   "source": [
    "**Reference Answer:**\n",
    "We notice that the model tends to produce long answers often several sentences while our ending is typically a sentence. To alleviate this problem, we can explicitly ask the model to produce one sentence ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b11027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_23 = []\n",
    "queries_23 = []\n",
    "reference_endings_23 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "860049ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1 METEOR Score: 13.89\n",
      "Query 2 METEOR Score: 44.45\n",
      "Query 3 METEOR Score: 26.52\n",
      "Query 4 METEOR Score: 37.92\n",
      "Query 5 METEOR Score: 17.99\n",
      "Query 6 METEOR Score: 54.37\n",
      "Query 7 METEOR Score: 23.55\n",
      "Query 8 METEOR Score: 12.74\n",
      "Query 9 METEOR Score: 16.13\n",
      "Query 10 METEOR Score: 16.67\n",
      "Average METEOR Score: 26.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "instruction = \"You are a creative storyteller. Generate a one-sentence ending of the given story.\"\n",
    "run(train_samples_sg, test_data_sg, 1, generations_23, queries_23, reference_endings_23,\n",
    "    instruction=instruction, task_name=\"2_3_intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba333f",
   "metadata": {},
   "source": [
    "<a name=\"23\"></a>\n",
    "### **Further Reading**\n",
    "\n",
    "In this exercise session, we played with different prompting techniques for in-context learning, however, all our methods were quite basic. In the past few years, the research community has developed various sophisticated prompting approaches that have been shown to elicit more from large language models. This field has grown so much that [*prompt engineering*](https://www.techtarget.com/searchenterpriseai/definition/AI-prompt-engineer) has become a legit job position. Some well-known prompting techniques are [Chain-of-thought](https://arxiv.org/abs/2201.11903), [Tree-of-thought](https://arxiv.org/abs/2305.10601), [Metacognitive prompting](https://arxiv.org/abs/2308.05342), [Self-consistency](https://arxiv.org/abs/2203.11171) and etc. You can find more information about all these techniques in this [prompting guide](https://www.promptingguide.ai). We encourage you to play with these techniques and compare your results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "mnlp-ex7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
