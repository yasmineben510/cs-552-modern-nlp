{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac671356",
   "metadata": {
    "id": "ac671356"
   },
   "source": [
    "#  Exercise 7: In-Context Learning with GPT-4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc96df3",
   "metadata": {
    "id": "7fc96df3"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Exercise Description**\n",
    "In this exercise, you will investigate in-context learning using OpenAI GPT-4o mini model. \n",
    "\n",
    "This exercise contains two parts.\n",
    "\n",
    "- In the first part, you will investigate in-context learning for classification based on a natural language inference (NLI) task.\n",
    "    \n",
    "- In the second part, you will investigate in-context learning for generation based on a story ending generation (SEG) task.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **[PART 1: In-Context Learning for Natural Language Inference](#1)**\n",
    "    - [1.1 Compare Different Shots](#11)\n",
    "    - [1.2 Effect of Neutral In-Context Examples](#12)\n",
    "    - [1.3 Play with Different Verbalizers](#13)\n",
    "    - [1.4 Task Instructions](#14)\n",
    "- **[PART 2: In-Context Learning for Story Ending Generation](#2)**\n",
    "    - [2.1 Zero-Shot Generation](#21)\n",
    "    - [2.2 Few-Shot Generation](#22)\n",
    "    - [2.3 Task Instructions](#23)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLBx5JbP0xNR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17844,
     "status": "ok",
     "timestamp": 1681740155337,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "iLBx5JbP0xNR",
    "outputId": "a40cc025-2d9a-481c-e282-52b14864a6e2"
   },
   "outputs": [],
   "source": [
    "# if you are using Google Colab, mount your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# switch to the path where you put the Exercise folder into\n",
    "%cd \"/content/drive/MyDrive/...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a720ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12760,
     "status": "ok",
     "timestamp": 1681740200735,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "a9a720ab",
    "outputId": "9c92e6e6-3b81-419b-8121-34e299d7e6aa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install numpy tqdm nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e14fd",
   "metadata": {
    "id": "623e14fd"
   },
   "source": [
    "You also need to install our **GPT wrapper** to interact with OpenAI GPT models for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c8ba5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8408,
     "status": "ok",
     "timestamp": 1681740214720,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "3d4c8ba5",
    "outputId": "d762dd09-aeb5-4731-8eae-50c520d80ba1"
   },
   "outputs": [],
   "source": [
    "!pip install gpt_wrapper-0.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0ca95",
   "metadata": {
    "id": "d4d0ca95"
   },
   "source": [
    "Import the required packages for this exercise, including our GPT wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81dd37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1681740221134,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "8d81dd37",
    "outputId": "f726397f-6cf5-435a-86a5-7d403c8d6423"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049b54b",
   "metadata": {
    "id": "3049b54b"
   },
   "source": [
    "To facilitate reproduction, we fix a random seed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca193e",
   "metadata": {
    "id": "acca193e"
   },
   "outputs": [],
   "source": [
    "seed = 233"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721322d2",
   "metadata": {
    "id": "721322d2"
   },
   "source": [
    "Set up the API access to our GPT wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd376243",
   "metadata": {
    "id": "cd376243"
   },
   "outputs": [],
   "source": [
    "gpt_wrapper.api_base = \"http://mnlp-backend-lb-1062233132.eu-central-1.elb.amazonaws.com\"\n",
    "gpt_wrapper.api_key = \"1067c253-7e95-42bc-9b57-fb03508f30dd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e49157",
   "metadata": {
    "id": "11e49157"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## **PART 1: In-Context Learning for Natural Language Inference**\n",
    "---\n",
    "\n",
    "In this part, you are going to use the GPT-4o mini model to solve the [natural language inference (NLI)](https://towardsdatascience.com/natural-language-inference-an-overview-57c0eecf6517) task based on in-context learning. For this task, model needs to classify the relation of two given sentences (premise and hypothesis) into three classes: entailment, neutral and contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed303c88",
   "metadata": {
    "id": "ed303c88"
   },
   "source": [
    "Here you can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-4o mini language model for classification (along with the gold answers for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee990d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1681740230765,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "f8ee990d",
    "outputId": "b35316c7-6c25-421a-bce4-8f77b5b4c5a4"
   },
   "outputs": [],
   "source": [
    "with open(\"nli_classification/train_classification.json\", \"r\") as f:\n",
    "    train_samples = json.load(f)\n",
    "with open(\"nli_classification/test_classification.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(\"Training Samples:\")\n",
    "print(train_samples[\"entailment\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caddef",
   "metadata": {
    "id": "01caddef"
   },
   "source": [
    "As you know by now, language models only produce a probability distribution over the vocabulary, so how many tokens to generate and how to select each token depends on the decoding algorithm. Since we are going to use GPT-4o mini model, we should get familiar with what type of decoding parameters OpenAI API offers:\n",
    "\n",
    "**max_tokens**: Maximum number of tokens to generate, by default generates as many as needed.\n",
    "\n",
    "**temperature**: Sampling temperature to use, between 0.0 and 2.0, default to 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "**top_p**: Nucleus sampling factor (alternative to sampling with temperature), between 0.0 and 1.0, default to 1.0. The model randomly samples from the tokens with top_p probability mass.\n",
    "\n",
    "**presence_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "\n",
    "**frequency_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HN3X2CRHdUmW",
   "metadata": {
    "id": "HN3X2CRHdUmW"
   },
   "source": [
    "For NLI task, we choose a small *max_tokens* because only the first non-space token generated by the model is used as the predicted class (i.e., verbalizer).\n",
    "\n",
    "**Question:**\n",
    "Why don't we simply set it to 1?\n",
    "\n",
    "We also change the *temperature* to 0 in order to let the model make deterministic classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bc651",
   "metadata": {
    "id": "ab3bc651"
   },
   "outputs": [],
   "source": [
    "model_args={\"max_tokens\": 5, \"temperature\": 0.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab97e5d",
   "metadata": {
    "id": "4ab97e5d"
   },
   "source": [
    "You will evaluate the model's NLI performance based on the accuracy and F1 scores on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6129f62",
   "metadata": {
    "id": "f6129f62"
   },
   "outputs": [],
   "source": [
    "def evaluate_nli(predictions, gold_labels, mapping):\n",
    "    \n",
    "    counter = np.zeros((3, 3))  # three-class confusion matrix\n",
    "    \n",
    "    # calculate the confusion matrix\n",
    "    for p, g in zip(predictions, gold_labels):\n",
    "        pid = mapping[p]\n",
    "        gid = mapping[g]\n",
    "        counter[gid][pid] += 1\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(counter)\n",
    "    \n",
    "    pred_sum = np.sum(counter, axis=0)  # total number of predictions on each class\n",
    "    gold_sum = np.sum(counter, axis=1)  # total number of test samples (gold labels) on each class\n",
    "    diag = np.diagonal(counter)  # total number of correct predictions on each class\n",
    "    \n",
    "    acc = np.sum(diag) / np.sum(counter)  # accuracy\n",
    "    \n",
    "    f1 = [0, 0, 0]\n",
    "    for cid in range(3):\n",
    "        precision = diag[cid] / pred_sum[cid] if pred_sum[cid] != 0 else 0  # precisions on each class\n",
    "        recall = diag[cid] / gold_sum[cid] if gold_sum[cid] != 0 else 0  # recalls on each class\n",
    "        f1[cid] = 2 * precision * recall / (precision + recall) if precision != 0 and recall != 0 else 0  # F1 scores on each class\n",
    "    \n",
    "    return acc, f1[0], f1[1], f1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886b1c6",
   "metadata": {
    "id": "d886b1c6"
   },
   "source": [
    "You will use the following function to perform GPT-4o mini inference on the NLI task based on in-context learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a878781",
   "metadata": {
    "id": "7a878781"
   },
   "outputs": [],
   "source": [
    "def gpt_nli(train_samples, test_data, shots, predictions, gold_answers,\n",
    "             instruction=None, default_class=\"neutral\", task_name=\"task\"):\n",
    "    \n",
    "    '''\n",
    "    train_samples: training data for sampling in-context examples\n",
    "    test_data: testing queries (with gold labels)\n",
    "    shots: number of in-context examples (shots) per class\n",
    "    predictions: cache for saving the model predictions\n",
    "    gold_answers: cache for saving gold answers\n",
    "    instruction: additional task instruction for prompting\n",
    "    default_class: default prediction class if the generated token is not among the verbalizers of three NLI classes\n",
    "    task_name: task name for creating chat sessions\n",
    "    '''\n",
    "    \n",
    "    # randomly sample in-context examples\n",
    "    examples = []\n",
    "    for nli_class, samples in train_samples.items():\n",
    "        few_shot_samples = random.sample(samples, shots[nli_class])\n",
    "        examples.extend(few_shot_samples)\n",
    "\n",
    "    random.shuffle(examples)  # randomly shuffle sampled in-context examples\n",
    "\n",
    "    for qid, query in enumerate(tqdm(test_data)):\n",
    "        \n",
    "        if qid < len(predictions):  # skip this query if its model prediction is already saved in cache\n",
    "            continue\n",
    "\n",
    "        # TODO: Construct the prompt using the in-context examples and the testing query\n",
    "        prompt = ...\n",
    "\n",
    "        # create a chat session using our GPT wrapper class Chat\n",
    "        chat = Chat.create(name=task_name+\"_\"+str(qid))\n",
    "        \n",
    "        # use the created chat session to query the GPT model with the input request,\n",
    "        # and get back model's output message\n",
    "        response = chat.ask(prompt, instruction=instruction, model_args=model_args)\n",
    "        \n",
    "        # model's output text is in the attribute \"content\",\n",
    "        # we use the first word of the generated text as the prediction\n",
    "        preds = response.content.strip().split()\n",
    "\n",
    "        if preds:\n",
    "            pred = preds[0].lower()\n",
    "        else:\n",
    "            pred = default_class\n",
    "        \n",
    "        # save the prediction in cache\n",
    "        if pred in train_samples.keys():\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            predictions.append(default_class)\n",
    "        \n",
    "        # save the gold answer in cache for evaluation\n",
    "        gold_answers.append(query[\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6619024",
   "metadata": {
    "id": "a6619024"
   },
   "source": [
    "You will run the following function to perform GPT inference and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692af4be",
   "metadata": {
    "id": "692af4be"
   },
   "outputs": [],
   "source": [
    "def run(train_samples, test_data, class_shots, mapping, predictions, gold_answers,\n",
    "        instruction=None, default_class=\"neutral\", task_name=\"none\"):\n",
    "    try:\n",
    "\n",
    "        gpt_nli(train_samples, test_data, class_shots, predictions, gold_answers,\n",
    "                 instruction=instruction, default_class=default_class, task_name=task_name)\n",
    "        \n",
    "        acc, f1_ent, f1_neu, f1_con = evaluate_nli(predictions, gold_answers, mapping)\n",
    "        macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n",
    "\n",
    "        print(f'Accuracy: {acc*100:.2f}% | F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')\n",
    "\n",
    "    except Exception as error:  # OpenAI ChatGPT endpoint may get stucked by too many queries from time to time\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ea96b",
   "metadata": {
    "id": "c90ea96b"
   },
   "source": [
    "<a name=\"11\"></a>\n",
    "### **1.1 Compare Different Shots**\n",
    "\n",
    "In this part, you will compare GPT-4o mini performances under different shots (number) of in-context examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25d6a1",
   "metadata": {
    "id": "2f25d6a1"
   },
   "source": [
    "#### 0-shot classification:\n",
    "\n",
    "Do not provide any in-context learning examples to the model.\n",
    "\n",
    "Create empty caches for saving model predictions and gold answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae867da",
   "metadata": {
    "id": "0ae867da"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921da49",
   "metadata": {
    "id": "3921da49"
   },
   "source": [
    "Run the inference and evaluation.\n",
    "\n",
    "**Note:** OpenAI ChatGPT endpoint may sometimes get stucked by too many queries. If running the following cell gets stucked, just re-run it, and inference will continue from the stucked query. However, do not re-run the above cell for creating the caches, which will clear the already saved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa56d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22585,
     "status": "ok",
     "timestamp": 1681740282153,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "78fa56d7",
    "outputId": "1eea57e7-128d-4d68-e877-b3910de50084"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 0, \"neutral\": 0, \"contradiction\": 0}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158536b5",
   "metadata": {
    "id": "158536b5"
   },
   "source": [
    "#### 1-shot per class:\n",
    "\n",
    "For each class, provide 1 in-context learning example sampled from the training data.\n",
    "\n",
    "Clear the caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd54af4",
   "metadata": {
    "id": "6bd54af4"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cb520",
   "metadata": {
    "id": "a43cb520"
   },
   "source": [
    "Re-run the inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544c0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23085,
     "status": "ok",
     "timestamp": 1681740311281,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "f544c0fd",
    "outputId": "655c95a4-1be4-45f0-d519-8fda494f85a1"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2eb071",
   "metadata": {
    "id": "1f2eb071"
   },
   "source": [
    "#### 2-shot per class:\n",
    "\n",
    "Try 2 in-context learning examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae49ee1",
   "metadata": {
    "id": "3ae49ee1"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c7eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23763,
     "status": "ok",
     "timestamp": 1681740340798,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "143c7eba",
    "outputId": "12d893fb-401c-433a-fd47-b77128db9275"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 2, \"neutral\": 2, \"contradiction\": 2}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f41c3",
   "metadata": {
    "id": "ae6f41c3"
   },
   "source": [
    "#### 3-shot per class:\n",
    "\n",
    "Try 3 in-context learning examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86dc6c2",
   "metadata": {
    "id": "c86dc6c2"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32278700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22961,
     "status": "ok",
     "timestamp": 1681740372517,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "32278700",
    "outputId": "a8c0c8e6-ccb2-4912-d3d7-672f0ebe05a9"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 3, \"neutral\": 3, \"contradiction\": 3}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca3308",
   "metadata": {
    "id": "fcca3308"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Can model handle well the NLI task without in-context examples for learning (i.e., under the 0-shot setting)?\n",
    "2. Which class are the in-context examples most helpful for detecting? and most helpless?\n",
    "3. Is the more in-context examples the better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87231e",
   "metadata": {
    "id": "ae87231e"
   },
   "source": [
    "<a name=\"12\"></a>\n",
    "### **1.2 Effect of Neutral In-Context Examples**\n",
    "\n",
    "Try 3-shot in-context examples on the entailment and contradictions classes, but do not provide any examples on the neutral class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d78c89",
   "metadata": {
    "id": "f6d78c89"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae734042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23782,
     "status": "ok",
     "timestamp": 1681740411286,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "ae734042",
    "outputId": "16e483c8-122e-48bb-bd26-74d371cb136f"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots = {\"entailment\": 3, \"neutral\": 0, \"contradiction\": 3}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef6a43",
   "metadata": {
    "id": "e8ef6a43"
   },
   "source": [
    "**Question:** Do you observe any difference here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c56bf9",
   "metadata": {
    "id": "77c56bf9"
   },
   "source": [
    "<a name=\"13\"></a>\n",
    "### **1.3 Play with Different Verbalizers**\n",
    "\n",
    "In this part, you will try to use different verbalizers for this NLI classification task. Verbalizers are mapping functions that map the numeric class labels such as 0, 1 and 2 to human-readable class labels such \"entailment\", \"neutral\" and \"contradiction\". However, this mapping is by no means the only correct one and in theory, we can use any mapping we want. So, next instead of using *entailment*, *neutral* and *contradiction*, you will try the following two alternatives:\n",
    "\n",
    "- *positive*, *unrelated* and *negative*\n",
    "- *a*, *b* and *c*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7e4f2",
   "metadata": {
    "id": "f9f7e4f2"
   },
   "source": [
    "Build data with the above two different verbalizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f33a23",
   "metadata": {
    "id": "88f33a23"
   },
   "outputs": [],
   "source": [
    "mapping_to_pun = {\"entailment\": \"positive\", \"neutral\": \"unrelated\", \"contradiction\": \"negative\"}\n",
    "train_samples_pun = {\"positive\": [], \"unrelated\": [], \"negative\": []}\n",
    "test_data_pun = []\n",
    "\n",
    "mapping_to_abc = {\"entailment\": \"a\", \"neutral\": \"b\", \"contradiction\": \"c\"}\n",
    "train_samples_abc = {\"a\": [], \"b\": [], \"c\": []}\n",
    "test_data_abc = []\n",
    "\n",
    "for nli_class, samples in train_samples.items():\n",
    "    # TODO: Populate the new training data for the two new tasks where the NLI classes are verbalized as \"positive\", \"unrelated\", and \"negative\",\n",
    "    # and as \"a\", \"b\", and \"c\", respectively\n",
    "    ...\n",
    "    \n",
    "for query in test_data:\n",
    "    # TODO: Populate the new testing data for the two new tasks where the gold answers are verbalized as \"positive\", \"unrelated\", and \"negative\",\n",
    "    # and as \"a\", \"b\", and \"c\", respectively\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11b99e",
   "metadata": {
    "id": "fc11b99e"
   },
   "source": [
    "You can take a glance of the processed training and testing data with different verbalizers.\n",
    "\n",
    "Data with verbalizers *positive*, *unrelated* and *negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7c0de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1681740449664,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "7ae7c0de",
    "outputId": "29004591-1864-44f3-e250-6bbbbba1cd03"
   },
   "outputs": [],
   "source": [
    "print(\"Training Samples:\")\n",
    "print(train_samples_pun[\"positive\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_pun[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data_pun[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4783fca",
   "metadata": {
    "id": "a4783fca"
   },
   "source": [
    "Data with verbalizers *a*, *b* and *c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64c3cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681740451809,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "2f64c3cb",
    "outputId": "39cfc7d3-453f-4b17-d6dd-9faca32ff8f8"
   },
   "outputs": [],
   "source": [
    "print(\"Training Samples:\")\n",
    "print(train_samples_abc[\"a\"][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_abc[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gold Answer:\")\n",
    "print(test_data_abc[0][\"gold_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30fd47",
   "metadata": {
    "id": "eb30fd47"
   },
   "source": [
    "#### Re-do the classification with new verbalizers.\n",
    "\n",
    "Try verbalizers *positive*, *unrelated* and *negative* under the 2-shot setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b52710",
   "metadata": {
    "id": "14b52710"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842ede7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23845,
     "status": "ok",
     "timestamp": 1681740485335,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "3842ede7",
    "outputId": "c83cffae-6cfd-4d6d-9894-eca326ba7e12"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots_pun = {\"positive\": 2, \"unrelated\": 2, \"negative\": 2}\n",
    "mapping_pun = {\"positive\": 0, \"unrelated\": 1, \"negative\": 2}\n",
    "\n",
    "run(train_samples_pun, test_data_pun, class_shots_pun, mapping_pun,\n",
    "    predictions, gold_answers, default_class=\"unrelated\", task_name=\"1_3_pun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada009a",
   "metadata": {
    "id": "5ada009a"
   },
   "source": [
    "Try verbalizers *a*, *b* and *c* under the 2-shot setting in 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efccae",
   "metadata": {
    "id": "f1efccae"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8194f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9311,
     "status": "ok",
     "timestamp": 1681740556153,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "df8194f6",
    "outputId": "d90bc77a-43cb-42b3-ab1b-df2daf9ccedf"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class_shots_abc = {\"a\": 2, \"b\": 2, \"c\": 2}\n",
    "mapping_abc = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "\n",
    "run(train_samples_abc, test_data_abc, class_shots_abc, mapping_abc,\n",
    "    predictions, gold_answers, default_class=\"b\", task_name=\"1_3_abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f384d",
   "metadata": {
    "id": "2c6f384d"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Are new verbalizers better or worse than the original ones?\n",
    "2. What do you think could be the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c62887",
   "metadata": {
    "id": "77c62887"
   },
   "source": [
    "<a name=\"14\"></a>\n",
    "### **1.4 Task Instructions**\n",
    "\n",
    "So far, we have only provided the model with a few examples of the NLI task, however, we didn't tell the model explicitly what the task is! But we also know that these models are self-supervised with a lot of data to follow human instructions to solve tasks such as \"Translate English to French\" or \"Answer in the style of Shakespeare\". \n",
    "\n",
    "In this part, you will try to come up with high-level task instructions/descriptions that can help the model understand the NLI task better and improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123b9cb",
   "metadata": {
    "id": "f123b9cb"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "gold_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252537f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23221,
     "status": "ok",
     "timestamp": 1681740593777,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "8252537f",
    "outputId": "6ca69e25-0d69-4b88-ec62-9b13fa9dc992"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TODO: Come up with a simple task description for the NLI task. You can also consider adding a role-playing element to the task description.\n",
    "instruction = ...\n",
    "class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n",
    "mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "run(train_samples, test_data, class_shots, mapping,\n",
    "    predictions, gold_answers, instruction=instruction, task_name=\"1_4_intro1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d0159",
   "metadata": {
    "id": "f17d0159"
   },
   "source": [
    "**Question:** What kind of instruction help improve the model performance and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f6131",
   "metadata": {
    "id": "5c8f6131"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## **PART 2: In-Context Learning for Story Ending Generation**\n",
    "---\n",
    "\n",
    "In this part, you will switch to using the GPT-4o mini model to solve the story ending generation (SEG) task based on in-context learning. For this task, model is given four lines of story plot and needs to generate the fifth line of the story plot as an ending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52dacd",
   "metadata": {
    "id": "2f52dacd"
   },
   "source": [
    "You can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-4o mini language model for story completion (along with the reference story ending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31a9bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1681740778880,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "2d31a9bd",
    "outputId": "9fa6d309-b0d7-4f2f-8f73-5647ff995391"
   },
   "outputs": [],
   "source": [
    "with open(\"story_generation/train_generation.json\", \"r\") as f:\n",
    "    train_samples_sg = json.load(f)\n",
    "with open(\"story_generation/test_generation.json\", \"r\") as f:\n",
    "    test_data_sg = json.load(f)\n",
    "\n",
    "print(\"Training Samples:\")\n",
    "print(train_samples_sg[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Query:\")\n",
    "print(test_data_sg[0][\"query\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Reference Story Ending:\")\n",
    "print(test_data_sg[0][\"reference_ending\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a56277",
   "metadata": {
    "id": "a8a56277"
   },
   "source": [
    "We will remove the limit on the maximum number of tokens since we are dealing with an open-ended text generation.\n",
    "\n",
    "We also change the *temperature* and *top_p* to 0.7 and 0.95 in order to enable the model's creativity and make it generate more diverse story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489af8bb",
   "metadata": {
    "id": "489af8bb"
   },
   "outputs": [],
   "source": [
    "model_args = {\"temperature\": 0.7, \"top_p\": 0.95, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fcc7f",
   "metadata": {
    "id": "d59fcc7f"
   },
   "source": [
    "You will evaluate the model generation performance based on [METEOR](https://aclanthology.org/W05-0909.pdf). This metric is originally proposed to evaluate machine translation quality, but later widely used in evaluating open-domain text (e.g., dialogues and stories) generation. It measures the alignments (i.e., matches) between words in the hypothesis to reference, by sequentially applying exact match, stemmed match and wordnet based synonym match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4a599",
   "metadata": {
    "id": "93f4a599"
   },
   "outputs": [],
   "source": [
    "def evaluate_seg(generation, reference):\n",
    "    ref_tokens = word_tokenize(reference)\n",
    "    gen_tokens = word_tokenize(generation)\n",
    "    score = meteor_score([ref_tokens], gen_tokens)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69d0d4",
   "metadata": {
    "id": "ff69d0d4"
   },
   "source": [
    "You will use the following function to perform GPT-4o mini generation on the SEG task based on in-context learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352adb17",
   "metadata": {
    "id": "352adb17"
   },
   "outputs": [],
   "source": [
    "def gpt_seg(train_samples, test_data, shot, generations, queries, reference_endings,\n",
    "             instruction=None, task_name=\"task\"):\n",
    "\n",
    "    '''\n",
    "    train_samples: training data for sampling in-context examples\n",
    "    test_data: testing queries (with reference story endings)\n",
    "    shot: number of in-context examples\n",
    "    generations: cache for saving the model generations\n",
    "    queries: cache for saving the input queries (i.e., four-line stories to be completed)\n",
    "    reference_endings: cache for reference story endings\n",
    "    instruction: additional task instruction for prompting\n",
    "    task_name: task name for creating chat sessions\n",
    "    '''\n",
    "    \n",
    "    # randomly sample in-context examples and shuffle them\n",
    "    examples = random.sample(train_samples, shot)\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    for qid, query in enumerate(tqdm(test_data)):\n",
    "        \n",
    "        if qid < len(generations):  # skip this query if its model generated story ending is already saved in cache\n",
    "            continue\n",
    "\n",
    "        # TODO: Construct the prompt using the in-context examples and the testing query\n",
    "        prompt = ...\n",
    "\n",
    "        # create a chat session using our GPT wrapper and query the model to get the story ending generation\n",
    "        chat = Chat.create(name=task_name+\"_\"+str(qid))\n",
    "        message = chat.ask(prompt, instruction=instruction, model_args=model_args)\n",
    "        \n",
    "        # save the model generation, story query and reference ending in caches\n",
    "        generations.append(message.content)\n",
    "        queries.append(query[\"query\"])\n",
    "        reference_endings.append(query[\"reference_ending\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28953b",
   "metadata": {
    "id": "5b28953b"
   },
   "source": [
    "You will run the following function to perform GPT-4o mini generation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e696a",
   "metadata": {
    "id": "fe1e696a"
   },
   "outputs": [],
   "source": [
    "def run(train_samples, test_data, shot, generations, queries, reference_endings, instruction=None, task_name=\"none\"):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        gpt_seg(train_samples, test_data, shot,\n",
    "                 generations, queries, reference_endings,\n",
    "                 instruction=instruction, task_name=task_name)\n",
    "\n",
    "        meteor_scores = []\n",
    "        print()\n",
    "\n",
    "        for qid, query in enumerate(queries):\n",
    "\n",
    "            meteor = evaluate_seg(generations[qid], reference_endings[qid])\n",
    "            print(\"Query \"+str(qid+1)+f' METEOR Score: {meteor*100:.2f}') \n",
    "\n",
    "            meteor_scores.append(meteor)\n",
    "\n",
    "        meteor_avg = sum(meteor_scores)/len(meteor_scores)\n",
    "        print(f'Average METEOR Score: {meteor_avg*100:.2f}')\n",
    "    \n",
    "    except Exception as error:  # OpenAI ChatGPT endpoint may get stucked by too many queries from time to time\n",
    "        \n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabe9f8",
   "metadata": {
    "id": "2aabe9f8"
   },
   "source": [
    "<a name=\"21\"></a>\n",
    "### **2.1 Zero-Shot Generation**\n",
    "\n",
    "Try 0-shot story ending generation (i.e., without any in-context examples).\n",
    "\n",
    "Create caches for saving model predictions, queries and reference story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff383572",
   "metadata": {
    "id": "ff383572"
   },
   "outputs": [],
   "source": [
    "generations_21 = []\n",
    "queries_21 = []\n",
    "reference_endings_21 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58400cf",
   "metadata": {
    "id": "c58400cf"
   },
   "source": [
    "Run the generation and evaluation.\n",
    "\n",
    "**Note:** Similar to Part 1, re-run the cell if it gets stucked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc252e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16075,
     "status": "ok",
     "timestamp": 1681741289721,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "bc252e8e",
    "outputId": "b72dc55f-e85a-4b7d-d1ea-195dcd8d6e69"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "run(train_samples_sg, test_data_sg, 0, generations_21, queries_21, reference_endings_21, task_name=\"2_1_shots0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028cff13",
   "metadata": {
    "id": "028cff13"
   },
   "source": [
    "You can print the saved caches and compare the quality of the reference and model-generated story endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b2dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1681741384000,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "6e0b2dd1",
    "outputId": "be324930-dd77-4254-8b2c-c4e8c7391a7a"
   },
   "outputs": [],
   "source": [
    "print(\"Queries:\\n\"+queries_21[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_21[0])\n",
    "print(\"Reference:\\n\"+reference_endings_21[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4813db",
   "metadata": {
    "id": "ae4813db"
   },
   "source": [
    "<a name=\"22\"></a>\n",
    "### **2.2 Few-Shot Generation**\n",
    "\n",
    "Try adding 5-shot in-context examples for this generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6f67a",
   "metadata": {
    "id": "89a6f67a"
   },
   "outputs": [],
   "source": [
    "generations_22 = []\n",
    "queries_22 = []\n",
    "reference_endings_22 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267b3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12302,
     "status": "ok",
     "timestamp": 1681741422976,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "5267b3c5",
    "outputId": "d0486b7b-7335-426a-979b-07db5765220a"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "run(train_samples_sg, test_data_sg, 5, generations_22, queries_22, reference_endings_22, task_name=\"2_2_shots5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be2268",
   "metadata": {
    "id": "44be2268"
   },
   "source": [
    "**Question:** Do few-shot examples improve the model's story ending generation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd1485",
   "metadata": {
    "id": "c9bd1485"
   },
   "source": [
    "You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028b9a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1681741525710,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "e028b9a4",
    "outputId": "1730a155-1fa6-47c9-8b9f-97a5b3a67edb"
   },
   "outputs": [],
   "source": [
    "print(\"Queries:\\n\"+queries_22[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_22[0])\n",
    "print(\"Reference:\\n\"+reference_endings_22[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276b631",
   "metadata": {
    "id": "2276b631"
   },
   "source": [
    "<a name=\"23\"></a>\n",
    "### **2.3 Add Instructions**\n",
    "\n",
    "Try 1-shot in-context learning with overall task instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcd744",
   "metadata": {
    "id": "e7bcd744"
   },
   "outputs": [],
   "source": [
    "generations_23 = []\n",
    "queries_23 = []\n",
    "reference_endings_23 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb203a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1681741562163,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "efb203a4",
    "outputId": "5ffac98a-d961-460a-e22c-165f08000f7b"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TODO: Come up with a simple task description for the story generation task. You can also consider adding a role-playing element to the task de√•scription.\n",
    "instruction = ...\n",
    "run(train_samples_sg, test_data_sg, 1, generations_23, queries_23, reference_endings_23,\n",
    "    instruction=instruction, task_name=\"2_3_intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd95e36",
   "metadata": {
    "id": "2cd95e36"
   },
   "source": [
    "**Question:** Does overall task instruction help improve the model's 1-shot story ending generation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314a4db",
   "metadata": {
    "id": "a314a4db"
   },
   "source": [
    "You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75485c15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681741590062,
     "user": {
      "displayName": "Silin Gao",
      "userId": "06208253100379244126"
     },
     "user_tz": -120
    },
    "id": "75485c15",
    "outputId": "0688a90b-d4b8-4edb-e70b-0f49656bbe6e"
   },
   "outputs": [],
   "source": [
    "print(\"Queries:\\n\"+queries_23[0])\n",
    "print(\"GPT-4o mini Generation:\\n\"+generations_23[0])\n",
    "print(\"Reference:\\n\"+reference_endings_23[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f18dce",
   "metadata": {},
   "source": [
    "**Question**: Depending on your instruction, do you observe any undesirable pattern in the model generations when 0 or 1 shot examples are given? How can we avoid this pattern in model generations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_23 = []\n",
    "queries_23 = []\n",
    "reference_endings_23 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860049ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "instruction = \"You are a creative storyteller. Generate a one-sentence ending of the given story.\"\n",
    "run(train_samples_sg, test_data_sg, 1, generations_23, queries_23, reference_endings_23,\n",
    "    instruction=instruction, task_name=\"2_3_intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba333f",
   "metadata": {},
   "source": [
    "<a name=\"23\"></a>\n",
    "### **Further Reading**\n",
    "\n",
    "In this exercise session, we played with different prompting techniques for in-context learning, however, all our methods were quite basic. In the past few years, the research community has developed various sophisticated prompting approaches that have been shown to elicit more from large language models. This field has grown so much that [*prompt engineering*](https://www.techtarget.com/searchenterpriseai/definition/AI-prompt-engineer) has become a legit job position. Some well-known prompting techniques are [Chain-of-thought](https://arxiv.org/abs/2201.11903), [Tree-of-thought](https://arxiv.org/abs/2305.10601), [Metacognitive prompting](https://arxiv.org/abs/2308.05342), [Self-consistency](https://arxiv.org/abs/2203.11171) and etc. You can find more information about all these techniques in this [prompting guide](https://www.promptingguide.ai). We encourage you to play with these techniques and compare your results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "mnlp-ex7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
